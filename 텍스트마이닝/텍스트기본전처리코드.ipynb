{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92af5437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaff07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table('./data/ratings_train.txt') \n",
    "train_data.dropna(inplace=True)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93e76a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt(doc):\n",
    "    doc = re.sub('[^ㄱ-ㅎ가-힣 ]', ' ', doc) # 한글이랑 공백(spacebar)만 남기고\n",
    "    doc = re.sub('\\s+', ' ', doc) # 다중 공백 하나의 공백으로 변환\n",
    "    doc = np.nan if doc == '' or doc == ' ' else doc # 전처리 과정에서 공백만 남는경우 삭제\n",
    "    return doc\n",
    "\n",
    "train_data['clean_doc'] = train_data['document'].apply(clean_txt)\n",
    "train_data.dropna(inplace=True)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb81e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "stw = pd.read_csv('./data/stopwords-ko.txt', header=None) # 불용어 사전(?) 활용\n",
    "stw_list = list(stw[0].values)\n",
    "tokenized_data = []\n",
    "for sen in train_data['clean_doc']:\n",
    "    temp_x = mecab.morphs(sen)\n",
    "    temp_x = [word for word in temp_x if not word in stw_list]\n",
    "    tokenized_data.append(temp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db08db60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d1f2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac416e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentences = [\n",
    "    'nice great best amazing', # 1\n",
    "    'stop lies', # 0\n",
    "    'pitiful nerd', # 0\n",
    "    'excellent work', # 1\n",
    "    'supreme quality', # 1\n",
    "    'bad', # 0\n",
    "    'highly respectable' # 1\n",
    "]\n",
    "y_train = [1,0,0,1,1,0,1] # 라벨도 부여해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3255bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(sentences)  # tokenizer를 sentences에 있는 단어에 대하여 fit\n",
    "vocab_size = len(t.word_index)+1 # zero_padding 떄문에 +1 여분 자리\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ab290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전에 훈련된 Word2vec 임베딩 가져오기\n",
    "\n",
    "import gensim.downloader as api\n",
    "path = api.load('word2vec-google-news-300', return_path=True)\n",
    "path\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "w2v_model = KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13878ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "def get_vector(word):\n",
    "    if word in w2v_model:\n",
    "        return w2v_model[word]\n",
    "    else:\n",
    "        return None\n",
    "for word, index in t.word_index.items():\n",
    "    # 단어와 맵핑되는 사전 훈련된 임베딩 벡터값\n",
    "    vector_value = get_vector(word)\n",
    "    if vector_value is not None:\n",
    "        embedding_matrix[index] = vector_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e315e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 훈련된 모델 사용하기\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, Input\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(max_len,), dtype='int32'))\n",
    "e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(X_train, y_train, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917fcf01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039e3dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
