{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f85259aa",
   "metadata": {},
   "source": [
    "# Bert 연습해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea18c33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T06:40:38.016600Z",
     "start_time": "2022-04-16T06:40:37.998620Z"
    }
   },
   "source": [
    "## 임베딩을 사용하는 방법\n",
    "1. 임베딩층(Embedding layer)을 랜덤 초기화하여 처음부터 학습하는 방법\n",
    "2. 방대한 데이터로 Word2Vec등과 같은 임베딩 알고리즘으로 사전에 학습된 임베딩 벡터를 가져와 사용하는 방법\n",
    "\n",
    "## 임베딩의 한계 및 해결책\n",
    "* 하나의 단어가 하나의 벡터값으로 맵핑되어 문맥을 고려하지 못하는 한계가 있음<br>\n",
    " -> 사전 훈련된 언어모델인 ELMo나 BERT 등으로 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0112ca",
   "metadata": {},
   "source": [
    "## Bert 전 모델들"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9058a6d0",
   "metadata": {},
   "source": [
    "### Semi-superviesed Sequence Learning<br>\n",
    ":LSTM 언어 모델을 학습하고나서 이렇게 학습한 LSTM을 텍스트 분류에 추가 학습하는 방법\n",
    "\n",
    "* 언어 모델은 주어진 텍스트로부터 이전 단어들로부터 다음 단어를 예측하도록 학습하므로 기본적으로 별도의 레이블이 부착되지 않은 텍스트 데이터로도 학습 가능(사전 훈련된 언어 모델의 강점은 학습 전 사람이 별도 레이블을 지정해줄 필요가 없다는 점)\n",
    "* 레이블이 없는 데이터로 학습된 LSTM과 가중치가 랜덤으로 초기화 된 LSTM 두 가지를 두고, 텍스트 분류와 같은 문제를 학습하여 사전 훈련된 언어 모델을 사용한 전자의 경우가 더 좋은 성능을 얻을 수 있다는 가능성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cb4ff3",
   "metadata": {},
   "source": [
    "![](https://wikidocs.net/images/page/108730/image1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0e89cd",
   "metadata": {},
   "source": [
    "### ELMo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e09df1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T07:26:57.315849Z",
     "start_time": "2022-04-16T07:26:57.308840Z"
    }
   },
   "source": [
    "* ELMo는 순방향 언어 모델과 역방향 언어 모델을 각각 따로 학습시킨 후에, \n",
    "이렇게 사전 학습된 언어 모델로부터 임베딩 값을 얻는다는 아이디어\n",
    "\n",
    "* 이러한 임베딩은 문맥에 따라서 임베딩 벡터값이 달라지므로, 기존 워드 임베딩인 Word2Vec이나 GloVe 등이 \n",
    "다의어를 구분할 수 없었던 문제점을 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5cbcfc",
   "metadata": {},
   "source": [
    "![](https://wikidocs.net/images/page/108730/image2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd78809",
   "metadata": {},
   "source": [
    "### GPT\n",
    "-> LSTM 대신 Transformer사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4b1741",
   "metadata": {},
   "source": [
    "![](https://wikidocs.net/images/page/108730/image3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5758b848",
   "metadata": {},
   "source": [
    "![](https://wikidocs.net/images/page/108730/image4.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6539305",
   "metadata": {},
   "outputs": [],
   "source": [
    "우측에 있는 양방향 언어 모델은 지금까지 본 적 없던 형태의 언어 모델입니다. \n",
    "실제로 이렇게 구현하는 경우는 거의 없는데 그 이유가 무엇일까요? \n",
    "가령, 양방향 LSTM을 이용해서 우측과 같은 언어 모델을 만들었다고 해봅시다. \n",
    "초록색 LSTM 셀은 순방향 언어 모델로 <sos>를 입력받아 I를 예측하고, 그 후에 am을 예측합니다. \n",
    "그런데 am을 예측할 때, 출력층은 주황색 LSTM 셀인 역방향 언어 모델의 정보도 함께 받고있습니다. \n",
    "그런데 am을 예측하는 시점에서 역방향 언어 모델이 이미 관측한 단어는 a, am, I 이렇게 3개의 단어입니다. \n",
    "이미 예측해야하는 단어를 역방향 언어 모델을 통해 미리 관측한 셈이므로 언어 모델은 일반적으로 양방향으로 구현하지 않습니다.\n",
    "\n",
    "하지만 문맥은 양방향이고 양방향을 구현하기 위해 마스크드 모델 탄생"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df3297e",
   "metadata": {},
   "source": [
    "## Bert(Bidirectional Encoder Representations from Transformers) 란?\n",
    " :BERT는 이전 챕터에서 배웠던 트랜스포머를 이용하여 구현되었으며,<br>\n",
    " 위키피디아(25억 단어)와 BooksCorpus(8억 단어)와 같은 레이블이 없는 텍스트 데이터로 사전 훈련된 언어 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c3c803",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T07:55:00.721664Z",
     "start_time": "2022-04-16T07:55:00.715680Z"
    }
   },
   "source": [
    "### Bert의 기본구조\n",
    ": 트랜스포머의 인코더를 쌓아올린 구조입니다. Base 버전에서는 총 12개를 쌓았으며, Large 버전에서는 총 24개를 쌓음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e105ff",
   "metadata": {},
   "source": [
    "![](https://wikidocs.net/images/page/35594/bartbase%EC%99%80large.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcbbe5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b678074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9572ebba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e3c8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4258d938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543fd802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614876db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d97e37a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463d007b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a07b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cbed50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e16293e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca04daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92af6e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5861db7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T05:17:11.507471Z",
     "start_time": "2022-04-16T05:16:42.913921Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
      "     ---------------------------------------- 79.7/79.7 KB 4.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
      "     ---------------------------------------- 4.0/4.0 MB 31.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.62.3)\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-1.11.0-cp39-cp39-win_amd64.whl (157.9 MB)\n",
      "     ------------------------------------- 157.9/157.9 MB 19.9 MB/s eta 0:00:00\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.12.0-cp39-cp39-win_amd64.whl (1.0 MB)\n",
      "     ---------------------------------------- 1.0/1.0 MB 62.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.20.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.0.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.7.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from sentence_transformers) (3.6.5)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 34.9 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
      "     ---------------------------------------- 77.9/77.9 KB ? eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.10.0.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.0)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
      "     ------------------------------------- 895.2/895.2 KB 55.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.8.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.26.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 42.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.4)\n",
      "Requirement already satisfied: click in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from nltk->sentence_transformers) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from nltk->sentence_transformers) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from scikit-learn->sentence_transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from torchvision->sentence_transformers) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.10.8)\n",
      "Requirement already satisfied: six in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.16.0)\n",
      "Building wheels for collected packages: sentence_transformers\n",
      "  Building wheel for sentence_transformers (setup.py): started\n",
      "  Building wheel for sentence_transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120748 sha256=38236afcebecd037357c30f959d0b002f24488725684c6337f64acb9b21754d3\n",
      "  Stored in directory: c:\\users\\bitcamp\\appdata\\local\\pip\\cache\\wheels\\2b\\11\\3b\\32a18fb9f2253b25d3d1a06f0a84e2d516e7efa19c8c71a283\n",
      "Successfully built sentence_transformers\n",
      "Installing collected packages: tokenizers, sentencepiece, torch, torchvision, sacremoses, huggingface-hub, transformers, sentence_transformers\n",
      "Successfully installed huggingface-hub-0.5.1 sacremoses-0.0.49 sentence_transformers-2.2.0 sentencepiece-0.1.96 tokenizers-0.12.1 torch-1.11.0 torchvision-0.12.0 transformers-4.18.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a357d713",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T05:30:36.404279Z",
     "start_time": "2022-04-16T05:30:32.337054Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27ac0ef6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T05:40:38.544488Z",
     "start_time": "2022-04-16T05:40:38.529544Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "         (Reuters) - Activision Blizzard (NASDAQ:ATVI) is cooperating with federal investigations into trading by friends of its chief executive shortly before the gaming company disclosed its sale to Microsoft Corp (NASDAQ:MSFT), it said in a securities filing on Friday.\n",
    "\n",
    "It received requests for information from the U.S. Securities and Exchange Commission and received a subpoena from a Department of Justice grand jury, the maker of \"Call of Duty\" said in an amended proxy filing.\n",
    "\n",
    "The requests \"appear to relate to their respective investigations into trading by third parties – including persons known to Activision Blizzard's CEO – in securities prior to the announcement of the proposed transaction,\" it said.\n",
    "\n",
    "Microsoft in January agreed to acquire Activision for $95 a share, or $68.7 billion in total, in the biggest video-gaming industry deal in history.\n",
    "\n",
    "The company did not name the parties, nor say whether the grand jury subpoena was directed at any employee.\n",
    "\n",
    "The filing did not disclose when it received the subpoena or the SEC request for information.\n",
    "\n",
    "Media moguls Barry Diller and David Geffen, and investor Alexander von Furstenberg, acquired share options after von Furstenberg met with Activision CEO Bobby Kotick and days before it disclosed the sale to Microsoft, the Wall Street Journal reported last month.\n",
    "\n",
    "\"Activision Blizzard has informed these authorities that it intends to be fully cooperative with these investigations,\" the company said.\n",
    "\n",
    "Diller told Reuters last month that none of the three had any knowledge about a potential acquisition and had acted on the belief that Activision was undervalued and had the potential for going private or being acquired.\n",
    "\n",
    "The amended proxy filing that included the information on its cooperation with the SEC and DOJ came after shareholders sued the company alleging omissions to a preliminary proxy on the sale.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e452a28d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T05:43:36.570488Z",
     "start_time": "2022-04-16T05:43:36.561511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigram 개수 : 487\n",
      "trigram 다섯개만 출력 : ['68 billion total' '68 billion total biggest'\n",
      " '68 billion total biggest video' '95 share 68' '95 share 68 billion'\n",
      " '95 share 68 billion total' 'acquire activision 95'\n",
      " 'acquire activision 95 share' 'acquire activision 95 share 68'\n",
      " 'acquired amended proxy']\n"
     ]
    }
   ],
   "source": [
    "# 3개의 단어 묶음인 단어구 추출\n",
    "\n",
    "n_gram_range = (3, 5)\n",
    "stop_words = \"english\"\n",
    "\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc]) #ngram_rangetuple (min_n, max_n), default=(1, 1)\n",
    "candidates = count.get_feature_names_out()\n",
    "\n",
    "print('trigram 개수 :', len(candidates))\n",
    "print('trigram 다섯개만 출력 :', candidates[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0188ab9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T05:46:01.903767Z",
     "start_time": "2022-04-16T05:45:49.174814Z"
    }
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding = model.encode([doc])\n",
    "candidate_embeddings = model.encode(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c32ac0b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T05:46:05.475216Z",
     "start_time": "2022-04-16T05:46:05.456294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['known activision blizzard ceo securities', 'von furstenberg met activision ceo', 'activision blizzard ceo securities', 'activision blizzard ceo securities prior', 'blizzard ceo securities prior announcement']\n"
     ]
    }
   ],
   "source": [
    "top_n = 5\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c478086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1cc53b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T05:47:34.786337Z",
     "start_time": "2022-04-16T05:47:34.768385Z"
    }
   },
   "source": [
    "## 다양한 키워드를 얻으려면?\n",
    "* Max Sum Similarity: 후보 간의 유사성을 최소화하면서 문서와의 후보 유사성을 극대화하고자 하는 것\n",
    "* Maximal Marginal Relevance :텍스트 요약 작업에서 중복을 최소화하고 결과의 다양성을 극대화하기 위해 노력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80b94da",
   "metadata": {},
   "source": [
    "## Max Sum Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c596f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def max_sum_sim(doc_embedding, candidate_embeddings, words, top_n, nr_candidates):\n",
    "    # 문서와 각 키워드들 간의 유사도\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    \n",
    "    # 각 키워드들 간의 유사도\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings, candidate_embeddings)\n",
    "    \n",
    "    # 코사인 유사도에 기반하여 키워드들 중 상위 top_n개의 단어를 pick\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [candidates[index] for index in words_idx]\n",
    "    distances_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349021c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce411a2f",
   "metadata": {},
   "source": [
    "## Maximal Marginal Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd0bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
    "\n",
    "    # 문서와 각 키워드들 간의 유사도가 적혀있는 리스트\n",
    "    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
    "\n",
    "    # 각 키워드들 간의 유사도\n",
    "    word_similarity = cosine_similarity(candidate_embeddings)\n",
    "\n",
    "    # 문서와 가장 높은 유사도를 가진 키워드의 인덱스를 추출.\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # keywords_idx = [2]\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "\n",
    "    # 가장 높은 유사도를 가진 키워드의 인덱스를 제외한 문서의 인덱스들\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # ==> candidates_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10 ... 중략 ...]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    # 최고의 키워드는 이미 추출했으므로 top_n-1번만큼 아래를 반복.\n",
    "    # ex) top_n = 5라면, 아래의 loop는 4번 반복됨.\n",
    "    for _ in range(top_n - 1):\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # MMR을 계산\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # keywords & candidates를 업데이트\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55c520d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56050d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 낮은 diversity\n",
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70ad487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b95742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 높은 diversity\n",
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64987a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549e1678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95adcdc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471fa0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b1e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd5ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c132192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b073ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bcbf9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2da7fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba932d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40eee72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4edb70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcecfe20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb3fd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2670b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011fb5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708a48fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
