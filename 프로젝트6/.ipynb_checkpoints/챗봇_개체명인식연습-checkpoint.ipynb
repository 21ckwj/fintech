{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a948b9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T08:12:11.525928Z",
     "start_time": "2022-06-14T08:12:11.159799Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2283b5d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T05:20:25.321535Z",
     "start_time": "2022-06-14T05:20:24.612669Z"
    }
   },
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/12.%20RNN%20Sequence%20Labeling/dataset/train.txt\", filename=\"train.txt\")\n",
    "\n",
    "f = open('train.txt', 'r')\n",
    "tagged_sentences = []\n",
    "sentence = []\n",
    "\n",
    "for line in f:\n",
    "    if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\n\":\n",
    "        if len(sentence) > 0:\n",
    "            tagged_sentences.append(sentence)\n",
    "            sentence = []\n",
    "        continue\n",
    "    splits = line.split(' ') # 공백을 기준으로 속성을 구분한다.\n",
    "    splits[-1] = re.sub(r'\\n', '', splits[-1]) # 줄바꿈 표시 \\n을 제거한다.\n",
    "    word = splits[0].lower() # 단어들은 소문자로 바꿔서 저장한다.\n",
    "    sentence.append([word, splits[-1]]) # 단어와 개체명 태깅만 기록한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "969c8b10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T05:21:39.239741Z",
     "start_time": "2022-06-14T05:21:39.221625Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['eu', 'B-ORG'],\n",
       " ['rejects', 'O'],\n",
       " ['german', 'B-MISC'],\n",
       " ['call', 'O'],\n",
       " ['to', 'O'],\n",
       " ['boycott', 'O'],\n",
       " ['british', 'B-MISC'],\n",
       " ['lamb', 'O'],\n",
       " ['.', 'O']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d595358",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T05:28:13.735933Z",
     "start_time": "2022-06-14T05:28:13.686033Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences, ner_tags = [], [] \n",
    "for tagged_sentence in tagged_sentences: # 14,041개의 문장 샘플을 1개씩 불러온다.\n",
    "    sentence, tag_info = zip(*tagged_sentence) # 각 샘플에서 단어들은 sentence에 개체명 태깅 정보들은 tag_info에 저장.\n",
    "    sentences.append(list(sentence)) # 각 샘플에서 단어 정보만 저장한다.\n",
    "    ner_tags.append(list(tag_info)) # 각 샘플에서 개체명 태깅 정보만 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0719b373",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T05:28:25.978621Z",
     "start_time": "2022-06-14T05:28:25.973635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫번째 샘플의 문장 : ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n",
      "첫번째 샘플의 레이블 : ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print('첫번째 샘플의 문장 :',sentences[0])\n",
    "print('첫번째 샘플의 레이블 :',ner_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e2e1f9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T05:29:00.603074Z",
     "start_time": "2022-06-14T05:29:00.597090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['only', 'france', 'and', 'britain', 'backed', 'fischler', \"'s\", 'proposal', '.']\n",
      "['O', 'B-LOC', 'O', 'B-LOC', 'O', 'B-PER', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[12])\n",
    "print(ner_tags[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940a4f06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58ed9df7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T05:54:09.695755Z",
     "start_time": "2022-06-14T05:54:09.382324Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('train.txt', <http.client.HTTPMessage at 0x2887e268760>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/12.%20RNN%20Sequence%20Labeling/dataset/train.txt\", filename=\"train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f4093d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02a0ea5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T07:35:35.607879Z",
     "start_time": "2022-06-14T07:35:35.590923Z"
    }
   },
   "outputs": [],
   "source": [
    "from konlpy import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d491b0bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T07:48:26.344733Z",
     "start_time": "2022-06-14T07:48:22.664013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting customized_konlpy\n",
      "  Downloading customized_konlpy-0.0.64-py3-none-any.whl (881 kB)\n",
      "     ------------------------------------- 881.5/881.5 kB 18.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: Jpype1>=0.6.1 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from customized_konlpy) (1.3.0)\n",
      "Requirement already satisfied: konlpy>=0.4.4 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from customized_konlpy) (0.6.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from konlpy>=0.4.4->customized_konlpy) (4.6.3)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (from konlpy>=0.4.4->customized_konlpy) (1.19.5)\n",
      "Installing collected packages: customized_konlpy\n",
      "Successfully installed customized_konlpy-0.0.64\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "pip install customized_konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8d18db3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T07:53:03.968200Z",
     "start_time": "2022-06-14T07:53:03.943268Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Hannanum' from 'ckonlpy.tag' (C:\\Users\\bitcamp\\anaconda3\\lib\\site-packages\\ckonlpy\\tag\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16700/4018600383.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mckonlpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTwitter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mHannanum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtwitter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTwitter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Hannanum' from 'ckonlpy.tag' (C:\\Users\\bitcamp\\anaconda3\\lib\\site-packages\\ckonlpy\\tag\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from ckonlpy.tag import Twitter\n",
    "twitter = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38aba7d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T07:51:48.050346Z",
     "start_time": "2022-06-14T07:51:48.038378Z"
    }
   },
   "outputs": [],
   "source": [
    "twitter.add_dictionary('은경이', 'Noun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f747cb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T07:51:48.411710Z",
     "start_time": "2022-06-14T07:51:48.391150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('은경이', 'Noun'),\n",
       " ('는', 'Josa'),\n",
       " ('사무실', 'Noun'),\n",
       " ('로', 'Josa'),\n",
       " ('갔습니다', 'Verb'),\n",
       " ('.', 'Punctuation')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.pos('은경이는 사무실로 갔습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9dd00edb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T08:11:17.867690Z",
     "start_time": "2022-06-14T08:11:17.687348Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/chatbot_tag_train.txt', <http.client.HTTPMessage at 0x2887fa0a220>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/keiraydev/chatbot/master/models/ner/ner_train.txt\", filename=\"./data/chatbot_tag_train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbab39f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T08:16:33.307273Z",
     "start_time": "2022-06-14T08:16:33.175771Z"
    }
   },
   "outputs": [],
   "source": [
    "f = open('./data/chatbot_tag_train.txt','r',encoding='utf-8')\n",
    "\n",
    "for line in f:\n",
    "    line = f.readline()\n",
    "    if line \n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbbad96",
   "metadata": {},
   "source": [
    "# 챗봇 깃허브 개체명 인식모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e1e9811",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T08:35:01.780026Z",
     "start_time": "2022-06-14T08:35:01.768058Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_13712/3395680838.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\bitcamp\\AppData\\Local\\Temp/ipykernel_13712/3395680838.py\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    from utils. import Preprocess\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from utils. import Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1732ef5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T08:34:51.925024Z",
     "start_time": "2022-06-14T08:34:46.807542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (22.1)\n",
      "Collecting pip\n",
      "  Downloading pip-22.1.2-py3-none-any.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 34.4 MB/s eta 0:00:00Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.1\n",
      "    Uninstalling pip-22.1:\n",
      "      Successfully uninstalled pip-22.1\n",
      "Successfully installed pip-22.1.2\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b3967d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T08:34:58.247773Z",
     "start_time": "2022-06-14T08:34:54.934393Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: utils in c:\\users\\bitcamp\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc24956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 파일 불러오기\n",
    "def read_file(file_name):\n",
    "    sents = []\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for idx, l in enumerate(lines):\n",
    "            if l[0] == ';' and lines[idx + 1][0] == '$':\n",
    "                this_sent = []\n",
    "            elif l[0] == '$' and lines[idx - 1][0] == ';':\n",
    "                continue\n",
    "            elif l[0] == '\\n':\n",
    "                sents.append(this_sent)\n",
    "            else:\n",
    "                this_sent.append(tuple(l.split()))\n",
    "    return sents\n",
    "\n",
    "\n",
    "p = Preprocess(word2index_dic='../../train_tools/dict/chatbot_dict.bin',\n",
    "               userdic='../../utils/user_dic.tsv')\n",
    "\n",
    "# 학습용 말뭉치 데이터를 불러옴\n",
    "corpus = read_file('ner_train.txt')\n",
    "\n",
    "# 말뭉치 데이터에서 단어와 BIO 태그만 불러와 학습용 데이터셋 생성\n",
    "sentences, tags = [], []\n",
    "for t in corpus:\n",
    "    tagged_sentence = []\n",
    "    sentence, bio_tag = [], []\n",
    "    for w in t:\n",
    "        tagged_sentence.append((w[1], w[3]))\n",
    "        sentence.append(w[1])\n",
    "        bio_tag.append(w[3])\n",
    "    \n",
    "    sentences.append(sentence)\n",
    "    tags.append(bio_tag)\n",
    "\n",
    "\n",
    "print(\"샘플 크기 : \\n\", len(sentences))\n",
    "print(\"0번 째 샘플 단어 시퀀스 : \\n\", sentences[0])\n",
    "print(\"0번 째 샘플 bio 태그 : \\n\", tags[0])\n",
    "print(\"샘플 단어 시퀀스 최대 길이 :\", max(len(l) for l in sentences))\n",
    "print(\"샘플 단어 시퀀스 평균 길이 :\", (sum(map(len, sentences))/len(sentences)))\n",
    "\n",
    "# 토크나이저 정의\n",
    "tag_tokenizer = preprocessing.text.Tokenizer(lower=False) # 태그 정보는 lower=False 소문자로 변환하지 않는다.\n",
    "tag_tokenizer.fit_on_texts(tags)\n",
    "\n",
    "# 단어사전 및 태그 사전 크기\n",
    "vocab_size = len(p.word_index) + 1\n",
    "tag_size = len(tag_tokenizer.word_index) + 1\n",
    "print(\"BIO 태그 사전 크기 :\", tag_size)\n",
    "print(\"단어 사전 크기 :\", vocab_size)\n",
    "\n",
    "# 학습용 단어 시퀀스 생성\n",
    "x_train = [p.get_wordidx_sequence(sent) for sent in sentences]\n",
    "y_train = tag_tokenizer.texts_to_sequences(tags)\n",
    "\n",
    "index_to_ner = tag_tokenizer.index_word # 시퀀스 인덱스를 NER로 변환 하기 위해 사용\n",
    "index_to_ner[0] = 'PAD'\n",
    "\n",
    "# 시퀀스 패딩 처리\n",
    "max_len = 40\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, padding='post', maxlen=max_len)\n",
    "y_train = preprocessing.sequence.pad_sequences(y_train, padding='post', maxlen=max_len)\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 8:2의 비율로 분리\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train,\n",
    "                                                    test_size=.2,\n",
    "                                                    random_state=1234)\n",
    "\n",
    "# 출력 데이터를 one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=tag_size)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=tag_size)\n",
    "\n",
    "print(\"학습 샘플 시퀀스 형상 : \", x_train.shape)\n",
    "print(\"학습 샘플 레이블 형상 : \", y_train.shape)\n",
    "print(\"테스트 샘플 시퀀스 형상 : \", x_test.shape)\n",
    "print(\"테스트 샘플 레이블 형상 : \", y_test.shape)\n",
    "\n",
    "\n",
    "# 모델 정의 (Bi-LSTM)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=30, input_length=max_len, mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.25)))\n",
    "model.add(TimeDistributed(Dense(tag_size, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.01), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=10)\n",
    "\n",
    "print(\"평가 결과 : \", model.evaluate(x_test, y_test)[1])\n",
    "model.save('ner_model.h5')\n",
    "\n",
    "\n",
    "# 시퀀스를 NER 태그로 변환\n",
    "def sequences_to_tag(sequences):  # 예측값을 index_to_ner를 사용하여 태깅 정보로 변경하는 함수.\n",
    "    result = []\n",
    "    for sequence in sequences:  # 전체 시퀀스로부터 시퀀스를 하나씩 꺼낸다.\n",
    "        temp = []\n",
    "        for pred in sequence:  # 시퀀스로부터 예측값을 하나씩 꺼낸다.\n",
    "            pred_index = np.argmax(pred)  # 예를 들어 [0, 0, 1, 0 ,0]라면 1의 인덱스인 2를 리턴한다.\n",
    "            temp.append(index_to_ner[pred_index].replace(\"PAD\", \"O\"))  # 'PAD'는 'O'로 변경\n",
    "        result.append(temp)\n",
    "    return result\n",
    "\n",
    "\n",
    "# f1 스코어 계산을 위해 사용\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "\n",
    "# 테스트 데이터셋의 NER 예측\n",
    "y_predicted = model.predict(x_test)\n",
    "pred_tags = sequences_to_tag(y_predicted) # 예측된 NER\n",
    "test_tags = sequences_to_tag(y_test)    # 실제 NER\n",
    "\n",
    "# F1 평가 결과\n",
    "print(classification_report(test_tags, pred_tags))\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c2d479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaec0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
